# TODO:
# - Introduce .env file for configurations
services:
  # intel gpu top
  intel_gpu_top:
    extends:
      file: base.yml
      service: base_igpu
    image: ghcr.io/amidg/intel_gpu_tools:latest
    container_name: intel_gpu_top
    profiles:
      - igpu_top
    restart: always

  # amd gaussian splat using pytorch
  amd_splat:
    profiles:
      - amd_splat
    extends:
      file: base.yml
      service: base_gui
    image: rocm/pytorch:rocm6.3.4_ubuntu22.04_py3.10_pytorch_release_2.4.0
    container_name: splat
    volumes:
      - /var/home/dmitrii/Projects/Misc/EasyVolcap:/app:z
    network_mode: host

  # amd ollama server
  amd_ollama:
    profiles:
      - amd_ollama
    extends:
      file: base.yml
      service: base_gpu
    image: docker.io/ollama/ollama:rocm
    container_name: ollama
    volumes:
      - ollama:/root/.ollama
    networks:
      - open-webui-network

  # intel ollama server
  # Mostly used by the neurowolf project
  igpu_ollama:
    profiles:
      - intel_ollama
    extends:
      file: base.yml
      service: base_igpu
    build:
      context: ./ai
      dockerfile: Dockerfile.ollama.intel
      target: base
    environment:
      - USE_XETLA=OFF
      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=0
      - SYCL_CACHE_PERSISTENT=1
      - ENABLE_SDP_FUSION=0
      - OLLAMA_NUM_GPU=999
      - no_proxy=localhost,127.0.0.1
      - ZES_ENABLE_SYSMAN=1
      - ONEAPI_DEVICE_SELECTOR=level_zero:0

  # open web-ui is the same for all GPUs
  open-webui:
    profiles:
      - nvidia_ollama
      - amd_ollama
      - intel_ollama
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - open-webui:/app/backend/data
    networks:
      - open-webui-network
    ports:
      - "3001:8080"

volumes:
  ollama:
    name: ollama
  open-webui:
    name: open-webui

networks:
  open-webui-network:
