# TODO:
# - Introduce .env file for configurations
services:
  # intel gpu top
  intel_gpu_top:
    extends:
      file: base.yml
      service: base_igpu
    container_name: intel_gpu_top
    profiles:
      - igpu_top
    image: ghcr.io/amidg/intel_gpu_tools:latest
    restart: always

  # intel igpu deepseek coder
  igpu_coder:
    extends:
      file: ./ai/base.yml
      service: igpu_ollama
    profiles:
      - igpu_coder
    container_name: igpu_coder
    environment:
      - LLM_MODEL=hf.co/tensorblock/deepseek-coder-7b-instruct-v1.5-GGUF
      - LLM_QUANT=Q4_K_M
      # large model that runs very slow on intel iGPU
      #- LLM_MODEL=hf.co/TheBloke/deepseek-coder-33B-instruct-GGUF
      #- LLM_QUANT=Q5_K_M

  # intel igpu deepseek coder
  igpu_chat:
    extends:
      file: ./ai/base.yml
      service: igpu_ollama
    profiles:
      - igpu_chat
    container_name: igpu_chat
    environment:
      - LLM_MODEL=hf.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF
      - LLM_QUANT=Q5_K_M
      # large model that runs very slow on intel iGPU
      #- LLM_MODEL=hf.co/TheBloke/deepseek-coder-33B-instruct-GGUF
      #- LLM_QUANT=Q5_K_M

  # intel igpu deepseek coder
  igpu_webui:
    extends:
      file: ./ai/base.yml
      service: igpu_webui
    profiles:
      - igpu_webui
    container_name: igpu_webui
    ports:
      - 3333:8080
