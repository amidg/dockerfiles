# TODO:
# - Introduce .env file for configurations
services:
  # nvidia ollama - docker only for now
  ollama:
    profiles:
      - nvda_ollama
    image: ollama/ollama
    container_name: ollama
    #ports:
    #  - "11434:11434"
    network_mode: host
    #networks:
    #  - open-webui-network
    volumes:
      - ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: always

  # intel gpu top
  intel_gpu_top:
    extends:
      file: base.yml
      service: base_igpu
    image: ghcr.io/amidg/intel_gpu_tools:latest
    container_name: intel_gpu_top
    profiles:
      - igpu_top
    restart: always

  # amd ollama server
  amd_ollama:
    profiles:
      - amd_ollama
      - ollama
    extends:
      file: base.yml
      service: base_gpu
    image: docker.io/ollama/ollama:rocm
    container_name: ollama
    volumes:
      - ollama:/root/.ollama
    networks:
      - open-webui-network
    ports:
      - "11434:11434"

  # intel ollama server
  # Mostly used by the neurowolf project
  igpu_ollama:
    profiles:
      - intel_ollama
    extends:
      file: base.yml
      service: base_igpu
    build:
      context: ./ai
      dockerfile: Dockerfile.ollama.intel
      target: base
    environment:
      - USE_XETLA=OFF
      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=0
      - SYCL_CACHE_PERSISTENT=1
      - ENABLE_SDP_FUSION=0
      - OLLAMA_NUM_GPU=999
      - no_proxy=localhost,127.0.0.1
      - ZES_ENABLE_SYSMAN=1
      - ONEAPI_DEVICE_SELECTOR=level_zero:0

  # open web-ui is the same for all GPUs
  open-webui:
    profiles:
      - ollama
      - nvda_ollama
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - OLLAMA_BASE_URL=http://172.17.0.1:11434
    volumes:
      - open-webui:/app/backend/data
    network_mode: host
    #networks:
    #  - open-webui-network
    #ports:
    #  - "3001:8080"

volumes:
  ollama:
    name: ollama
  open-webui:
    name: open-webui

networks:
  open-webui-network:
    driver: bridge
