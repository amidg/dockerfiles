# TODO:
# - Introduce .env file for configurations
services:
  # intel gpu top
  intel_gpu_top:
    extends:
      file: base.yml
      service: base_igpu
    container_name: intel_gpu_top
    profiles:
      - igpu_top
    image: ghcr.io/amidg/intel_gpu_tools:latest
    restart: always

  # amd ollama server
  amd_ollama:
    profiles:
      - amd_ollama
    extends:
      file: ./ai/base.yml
      service: amd_ollama
    container_name: ollama
    volumes:
      - ollama:/root/.ollama
    networks:
      - open-webui-network

  open-webui:
    profiles:
      - nvidia_ollama
      - amd_ollama
      - intel_ollama
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - open-webui:/app/backend/data
    networks:
      - open-webui-network
    ports:
      - "3001:8080"

  # ollama webui to be used alongside local LLM
  ollama_webui:
    image: ghcr.io/open-webui/open-webui:main
    profiles:
      - ai_webui
    container_name: ollama_webui
    networks:
      - ollama
    ports:
      - 3333:8080
    volumes:
      - ollama-webui:/app/backend/data
    environment:
      OLLAMA_BASE_URL: http://ollama:11435

volumes:
  ollama:
    name: ollama
  open-webui:
    name: open-webui

networks:
  open-webui-network:
