# TODO:
# - Introduce .env file for configurations
services:
  igpu_base: &igpu_base
    extends:
      file: ../base.yml
      service: base_igpu
    build:
      context: .
      dockerfile: Dockerfile.ollama.intel
      target: base
    network_mode: host
    environment:
      - USE_XETLA=OFF
      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=0
      - SYCL_CACHE_PERSISTENT=1
      - ENABLE_SDP_FUSION=0
      - OLLAMA_NUM_GPU=999
      - no_proxy=localhost,127.0.0.1
      - ZES_ENABLE_SYSMAN=1
      - ONEAPI_DEVICE_SELECTOR=level_zero:0
    tty: true

  # intel igpu ollama
  igpu_ollama:
    <<: *igpu_base
    entrypoint: >
      bash -c "/llm/ollama serve > /dev/null 2>&1"

  # intel igpu ollama with webui
  igpu_webui:
    <<: *igpu_base
    build:
      context: .
      dockerfile: Dockerfile.ollama.intel
      target: webui
