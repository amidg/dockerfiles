# TODO:
# - Introduce .env file for configurations
services:
  # Nvidia ollama

  # AMD GPU using rocm
  amd_ollama:
    image: docker.io/ollama/ollama:rocm
    devices:
      - /dev/kfd
      - /dev/dri
    security_opt:
      - seccomp:unconfined
    privileged: true
    cap_add:
      - SYS_PTRACE

  # Ollama using Intel integrated graphics Gen 11+
  # Mostly used by the neurowolf project
  igpu_ollama:
    extends:
      file: ../base.yml
      service: base_igpu
    build:
      context: .
      dockerfile: Dockerfile.ollama.intel
      target: base
    #network_mode: host
    environment:
      - USE_XETLA=OFF
      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=0
      - SYCL_CACHE_PERSISTENT=1
      - ENABLE_SDP_FUSION=0
      - OLLAMA_NUM_GPU=999
      - no_proxy=localhost,127.0.0.1
      - ZES_ENABLE_SYSMAN=1
      - ONEAPI_DEVICE_SELECTOR=level_zero:0
    tty: true
