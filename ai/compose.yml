# TODO:
# - Introduce .env file for configurations
services:
  #########################
  # BASE SERVICES
  #########################
  # base ollama server
  base_ollama: &base_ollama
    profiles:
      - null_profile
    volumes:
      - ollama:/root/.ollama
    networks:
      - open-webui-network
    ports:
      - 11434:11434

  #########################
  # OLLAMA SERVICES
  #########################
  # amd ollama server
  amd_ollama:
    <<: [*base_ollama]
    profiles:
      - amd_ollama
      - amd_ollama_server
    extends:
      file: ../base.yml
      service: base_amdgpu
    image: docker.io/ollama/ollama:rocm
    container_name: ollama_server
    environment:
      - no_proxy=localhost,127.0.0.1
      - OLLAMA_HOST=0.0.0.0

  # intel ollama server
  intel_ollama:
    <<: [*base_ollama]
    profiles:
      - intel_ollama
      - intel_ollama_server
    extends:
      file: ../base.yml
      service: base_mesa_gpu
    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
    container_name: ollama_server
    environment:
      - no_proxy=localhost,127.0.0.1
      - OLLAMA_HOST=0.0.0.0
      - DEVICE=Arc
      - OLLAMA_INTEL_GPU=true
      - OLLAMA_NUM_GPU=999
      - ZES_ENABLE_SYSMAN=1
    command: sh -c 'mkdir -p /llm/ollama && cd /llm/ollama && init-ollama && exec ./ollama serve'

  # open web-ui is the same for all GPUs
  open-webui:
    profiles:
      - webui
      - intel_ollama
      - amd_ollama
      - nvda_ollama
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - OLLAMA_BASE_URL=http://ollama_server:11434
    volumes:
      - open-webui:/app/backend/data
    networks:
      - open-webui-network
    ports:
      - 8080:8080

volumes:
  ollama:
    name: ollama
  open-webui:
    name: open-webui

networks:
  open-webui-network:
    driver: bridge
