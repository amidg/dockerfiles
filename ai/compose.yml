# TODO:
# - Introduce .env file for configurations
services:
  #########################
  # BASE SERVICES
  #########################
  # base ollama server
  base_ollama: &base_ollama
    profiles:
      - null_profile
    volumes:
      - ollama:/root/.ollama
    networks:
      - open-webui-network
    ports:
      - 11434:11434

  # base intel ollama server
  base_intel_ollama: &base_intel_ollama
    profiles:
      - null_profile
    extends:
      file: ../base.yml
      service: base_mesa_gpu
    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
    environment:
      - no_proxy=localhost,127.0.0.1
      - OLLAMA_HOST=0.0.0.0
      - DEVICE=Arc
      - OLLAMA_INTEL_GPU=true
      - OLLAMA_NUM_GPU=999
      - ZES_ENABLE_SYSMAN=1
    command: sh -c 'mkdir -p /llm/ollama && cd /llm/ollama && init-ollama && exec ./ollama serve'

  #########################
  # OLLAMA SERVICES
  #########################
  # amd ollama server
  #amd_ollama:
  #  <<: *base_ollama
  #  profiles:
  #    - amd_ollama
  #  extends:
  #    file: ../base.yml
  #    service: base_gpu
  #  image: docker.io/ollama/ollama:rocm
  #  container_name: ollama

  # intel ollama server
  intel_ollama:
    <<: [*base_ollama, *base_intel_ollama]
    profiles:
      - intel_ollama
    container_name: intel_ollama_server

  # open web-ui is the same for all GPUs
  open-webui:
    profiles:
      - webui
      - intel_ollama
      - amd_ollama
      - nvda_ollama
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - OLLAMA_BASE_URL=http://0.0.0.0:11434
    volumes:
      - open-webui:/app/backend/data
    networks:
      - open-webui-network
    ports:
      - 3001:8000

volumes:
  ollama:
    name: ollama
  open-webui:
    name: open-webui

networks:
  open-webui-network:
    driver: bridge
