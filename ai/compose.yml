# TODO:
# - Introduce .env file for configurations
services:
  # base ollama server
  base_ollama: &base_ollama
    profiles:
      - null_profile
    volumes:
      - ollama:/root/.ollama
    networks:
      - open-webui-network
    ports:
      - 11434:11434

  # amd ollama server
  #amd_ollama:
  #  <<: *base_ollama
  #  profiles:
  #    - amd_ollama
  #  extends:
  #    file: ../base.yml
  #    service: base_gpu
  #  image: docker.io/ollama/ollama:rocm
  #  container_name: ollama

  # intel ollama server
  # Credit: https://github.com/eleiton/ollama-intel-arc
  intel_ollama:
    <<: *base_ollama
    profiles:
      - intel_ollama
      - ollama
    extends:
      file: ../base.yml
      service: base_mesa_gpu
    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
    environment:
      - no_proxy=localhost,127.0.0.1
      - OLLAMA_HOST=0.0.0.0
      - DEVICE=Arc
      - OLLAMA_INTEL_GPU=true
      - OLLAMA_NUM_GPU=999
      - ZES_ENABLE_SYSMAN=1
    command: sh -c 'mkdir -p /llm/ollama && cd /llm/ollama && init-ollama && exec ./ollama serve'
    #environment:
    #  - USE_XETLA=OFF
    #  - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=0
    #  - SYCL_CACHE_PERSISTENT=1
    #  - ENABLE_SDP_FUSION=0
    #  - OLLAMA_NUM_GPU=999
    #  - no_proxy=localhost,127.0.0.1
    #  - ZES_ENABLE_SYSMAN=1
    #  - ONEAPI_DEVICE_SELECTOR=level_zero:0

  # open web-ui is the same for all GPUs
  open-webui:
    profiles:
      - webui
      - ollama
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - OLLAMA_BASE_URL=http://192.168.0.199:11434
    volumes:
      - open-webui:/app/backend/data
    networks:
      - open-webui-network
    ports:
      - 3001:3001

volumes:
  ollama:
    name: ollama
  open-webui:
    name: open-webui

networks:
  open-webui-network:
    driver: bridge
